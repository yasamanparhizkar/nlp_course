{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI4PH Short Course in NLP - Computer Assignment\n",
        "*10 December 2023*\n",
        "</br>\n",
        "*Author: Yasaman Parhizkar*\n",
        "\n",
        "**Description:**\n",
        "\n",
        "Create a Jupyter notebook that performs the following NLP in this order:\n",
        "\n",
        "1. Load the Brown Corpus from NLTK using paras().\n",
        "2. Remove punctuation and stopwords.\n",
        "3. Apply the lancaster stemmer.\n",
        "4. Print to the screen the top 10 words in terms of TF. Show the TF values as well.\n",
        "5. Print to the screen the top 10 words in terms of TF-IDF. Use the paragraphs as documents for calculating TF-IDF. Show the TF-IDF values as well.\n",
        "6. Use pos_tag() to tag each token.\n",
        "7. Print to the screen the 10 most common trigrams of word-tag pairs. Show their frequencies as well. Use nltk.trigrams().\n",
        "8. Please clean up your code before submission. Your code should not contain any rough work. Also, insert appropriate comments to make it easy to understand and follow your code.  \n",
        "\n",
        "This is a pass or fail assignment and you will receive a pass as long as your code performs the tasks above without any error. Submit your Jupyter notebook to Canvas by 9:59pm MT / 11:59pm ET on Sunday, December 10, 2023."
      ],
      "metadata": {
        "id": "qqVTZqr4wxbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install required packages\n",
        "!pip install nltk tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4trNwa-TwuEA",
        "outputId": "58c4cb98-f9f5-4d66-9a06-41f40b7b3814"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm, trange\n",
        "import math\n",
        "import operator\n",
        "\n",
        "# download the Brown Corpus and other required corpora\n",
        "nltk.download('brown')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "4G_qnQeuwuBj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "369f1b3a-8bd8-4bcd-874b-b3bece3c737b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the brown corpus\n",
        "brown_paras = brown.paras()\n",
        "print('Test Sentence:', \" \".join(brown_paras[0][0]), '\\n')\n",
        "\n",
        "# create a list of all punctuations\n",
        "punctuations = string.punctuation + '--``\\\"\\\"\\'\\''\n",
        "\n",
        "# remove punctuations\n",
        "brown_paras_clean = [[[token.lower() for token in sent if not token in punctuations] for sent in paraph] for paraph in tqdm(brown_paras, desc='Punctuation Removal')]\n",
        "print('\\nTest Sentence:', \" \".join(brown_paras_clean[0][0]), '\\n')\n",
        "\n",
        "# remove stopwords\n",
        "brown_paras_clean = [[[token for token in sent if not token in stopwords.words('english')] for sent in paraph] for paraph in tqdm(brown_paras_clean, desc='Stopword Removal')]\n",
        "print('\\nTest Sentence:', \" \".join(brown_paras_clean[0][0]), '\\n')\n",
        "\n",
        "# apply the lancaster stemmer\n",
        "lancaster = nltk.LancasterStemmer()\n",
        "lancaster_stems = [[[lancaster.stem(word) for word in sent] for sent in paraph] for paraph in tqdm(brown_paras_clean, desc='Lancaster Stemmer')]\n",
        "print('\\nTest Sentence:', \" \".join(lancaster_stems[0][0]), '\\n')"
      ],
      "metadata": {
        "id": "fMjVklA9wt-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e2ffc14-e015-4eba-a818-63bc57b6cea4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Sentence: The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place . \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Punctuation Removal: 100%|██████████| 15667/15667 [00:05<00:00, 2735.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Sentence: the fulton county grand jury said friday an investigation of atlanta's recent primary election produced no evidence that any irregularities took place \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stopword Removal: 100%|██████████| 15667/15667 [01:57<00:00, 132.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Sentence: fulton county grand jury said friday investigation atlanta's recent primary election produced evidence irregularities took place \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Lancaster Stemmer: 100%|██████████| 15667/15667 [00:10<00:00, 1486.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Sentence: fulton county grand jury said friday investig atlanta's rec prim elect produc evid irregul took plac \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute TF\n",
        "brown_tokens = [token for paraph in brown_paras_clean for sent in paraph for token in sent] # flatten the list of tokens\n",
        "tf = nltk.FreqDist(brown_tokens)\n",
        "tf.most_common()[:10]"
      ],
      "metadata": {
        "id": "8wE9QyctwtwS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d889d0c-bc51-4808-89f5-6049cbd6be3f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('one', 3292),\n",
              " ('would', 2714),\n",
              " ('said', 1961),\n",
              " ('new', 1635),\n",
              " ('could', 1601),\n",
              " ('time', 1598),\n",
              " ('two', 1412),\n",
              " ('may', 1402),\n",
              " ('first', 1361),\n",
              " ('like', 1292)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute TF-IDF\n",
        "tf_idf = {}\n",
        "ntokens = len(brown_tokens)\n",
        "ndocs = len(brown_paras_clean)\n",
        "\n",
        "n_tokens_computed = 100\n",
        "token_idx = 0\n",
        "\n",
        "for token in tqdm(tf, desc='TF-IDF Computation'):\n",
        "    count = 0\n",
        "    doc_idx = 0\n",
        "\n",
        "    while doc_idx < ndocs:\n",
        "        doc = [token for sent in brown_paras_clean[doc_idx] for token in sent]\n",
        "        if token in doc:\n",
        "            count += 1\n",
        "        doc_idx += 1\n",
        "\n",
        "    tf_idf[token] = tf[token] * math.log(ndocs / count)\n",
        "\n",
        "    # NOTE: Uncomment the next three lines to limit the number of considered tokens for faster runtime.\n",
        "    # if token_idx >= n_tokens_computed-1:\n",
        "    #   break\n",
        "    # token_idx += 1\n",
        "\n",
        "\n",
        "# print the words with the largest tf-idf values\n",
        "sorted_tf_idf = sorted(tf_idf.items(),\n",
        "                       key=operator.itemgetter(1),\n",
        "                       reverse=True)  # this is how you sort a dict\n",
        "sorted_tf_idf[:10]"
      ],
      "metadata": {
        "id": "5bUIANCbwthy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6da7dcf-b9cd-4b1c-d13e-e0ad92c5cd9e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TF-IDF Computation: 100%|██████████| 49637/49637 [49:50<00:00, 16.60it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('one', 5967.480472092773),\n",
              " ('would', 5714.329758469583),\n",
              " ('said', 4260.661866852177),\n",
              " ('new', 4097.735481700572),\n",
              " ('af', 4086.0738714671734),\n",
              " ('could', 3958.3297676189504),\n",
              " ('time', 3904.4523114557496),\n",
              " ('may', 3823.063295503012),\n",
              " ('two', 3595.191700677196),\n",
              " ('first', 3501.2731240146404)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# part-of-speach tagging\n",
        "tags = nltk.pos_tag(brown_tokens)\n",
        "tags[:10]"
      ],
      "metadata": {
        "id": "tJw1L5skwtdR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e99424-de72-4c3f-b287-54be51729823"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('fulton', 'NN'),\n",
              " ('county', 'NN'),\n",
              " ('grand', 'JJ'),\n",
              " ('jury', 'NN'),\n",
              " ('said', 'VBD'),\n",
              " ('friday', 'JJ'),\n",
              " ('investigation', 'NN'),\n",
              " (\"atlanta's\", 'NN'),\n",
              " ('recent', 'JJ'),\n",
              " ('primary', 'JJ')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the most common word-tag trigrams\n",
        "tf_wt_trigram = nltk.FreqDist(nltk.trigrams(tags))\n",
        "tf_wt_trigram.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6B44kbqwvb6",
        "outputId": "69e0fb67-4f6d-4689-aa06-6d7c4f19e402"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((('world', 'NN'), ('war', 'NN'), ('2', 'CD')), 35),\n",
              " ((('new', 'JJ'), ('york', 'NN'), ('city', 'NN')), 26),\n",
              " ((('new', 'JJ'), ('york', 'NN'), ('times', 'NNS')), 18),\n",
              " ((('government', 'NN'), ('united', 'VBD'), ('states', 'NNS')), 18),\n",
              " ((('basic', 'JJ'), ('wage', 'NN'), ('rate', 'NN')), 16),\n",
              " ((('a.', 'NN'), ('notte', 'NN'), ('jr.', 'NN')), 15),\n",
              " ((('notte', 'NN'), ('jr.', 'NN'), ('governor', 'NN')), 15),\n",
              " ((('per', 'IN'), ('capita', 'JJ'), ('income', 'NN')), 14),\n",
              " ((('world', 'NN'), ('war', 'NN'), ('1', 'CD')), 13),\n",
              " ((('index', 'NN'), ('words', 'NNS'), ('electronic', 'JJ')), 13)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}